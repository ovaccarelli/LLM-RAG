{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ovaccarelli/LLM-RAG/blob/main/notebooks/llm_rag_Open_Source_AI_Workshop_3.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0MJe6AusVtI"
      },
      "source": [
        "\n",
        "# üîß Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install all required Python packages for this workshop\n",
        "\n",
        "!pip install wget langchain langchain-community pypdf faiss-cpu sentence-transformers rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import wget\n",
        "from rich.console import Console\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.retrievers import BM25Retriever\n",
        "\n",
        "console = Console()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the \"data\" folder \n",
        "try:\n",
        "    import google.colab  # only exists on Colab\n",
        "    DATA_FOLDER = Path(\"data\")         # Colab path\n",
        "except ImportError:\n",
        "    DATA_FOLDER = Path(\"../data\")      # Local path\n",
        "os.makedirs(DATA_FOLDER, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPoWurPW_5gb"
      },
      "source": [
        "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKgX3jOJ7OZT"
      },
      "source": [
        "## 3. Construct the vectorstore\n",
        "\n",
        "In this step, we take the PDF documents and transform them into a searchable vector database.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Pdf file downloaded successfully.</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;32mPdf file downloaded successfully.\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create the \"data/PDFs\" folder if it doesn't exist\n",
        "PDF_FOLDER = DATA_FOLDER/\"PDFs\"\n",
        "os.makedirs(PDF_FOLDER, exist_ok=True)\n",
        "\n",
        "urls = [\n",
        "    \"https://raw.githubusercontent.com/ovaccarelli/LLM-RAG/main/data/PDFs/Open_Source_AI_workshop.pdf\",\n",
        "]\n",
        "\n",
        "# Download the PDFs\n",
        "for url in urls:\n",
        "    name = url.split(\"/\")[-1]\n",
        "    if not (PDF_FOLDER / name).is_file():\n",
        "        filename = wget.download(url, f\"data/PDFs/{name}\")\n",
        "console.print(\"Pdf file downloaded successfully.\", style=\"bold green\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0IUq-0eF1_x",
        "outputId": "babd1cf4-4e4e-492b-e52e-e72f0d2f8078"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1 PDF pages\n"
          ]
        }
      ],
      "source": [
        "# 1. Create a folder to store the vector index\n",
        "VECTORSTORES_DIR = PDF_FOLDER/\"vectorstores\"\n",
        "os.makedirs(VECTORSTORES_DIR, exist_ok=True)\n",
        "\n",
        "# 2. Use PyPDFDirectoryLoader to load every PDF page as a Document\n",
        "loader = PyPDFDirectoryLoader(PDF_FOLDER)\n",
        "documents = loader.load()\n",
        "\n",
        "# 3. Verify how many pages are loaded\n",
        "print(f\"Loaded {len(documents)} PDF pages\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U69xtJTpC0uk"
      },
      "source": [
        "### ‚úÇÔ∏è Split Documents into Chunks\n",
        "\n",
        "We break documents into smaller overlapping chunks using `RecursiveCharacterTextSplitter`.\n",
        "\n",
        "- `chunk_size`: The number of characters per chunk.\n",
        "\n",
        "- `chunk_overlap`: Ensures that we maintain context between chunks.\n",
        "\n",
        "This is crucial for preserving semantic meaning across sentences and paragraphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u2AIMgmMNHw",
        "outputId": "df3f34f8-2aaf-497f-ec9e-8e87df2db812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Split into 5 chunks\n"
          ]
        }
      ],
      "source": [
        "# Set chunk size (how many characters per chunk) and overlap\n",
        "CHUNK_SIZE = ...\n",
        "CHUNK_OVERLAP = ...\n",
        "\n",
        "# Initialize the text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=CHUNK_OVERLAP\n",
        ")\n",
        "\n",
        "# Split the loaded PDFs into smaller, overlapping chunks\n",
        "all_splits = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"‚úÖ Split into {len(all_splits)} chunks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- üìë Chunk 1 ---\n",
            "Open Source AI workshop \n",
            "May 8, 2025, 9:00 ‚Äì Bern University of Applied Science (BFH) \n",
            " \n",
            "LLM-RAG from Scratch: Crafting Intelligent AI Retrieval Systems \n",
            "Embark on a hands-on journey to build your very own Retrieval -Augmented Generation (LLM-RAG) \n",
            "system from scratch using open source AI models and cutting -edge tools. In this workshop, you‚Äôll \n",
            "learn how to integrate large language models with retrieval pipelines and dive into vast repositories\n",
            "\n",
            "--- üìë Chunk 2 ---\n",
            "of data, extracting the precise information you need to generate  clear, accurate, and contextually \n",
            "rich responses. \n",
            " \n",
            "During this Workshop, You Will: \n",
            "‚Ä¢ Master the Fundamentals:  Learn how the RAG framework combines retrieval and \n",
            "generation to produce responses that are both context-aware and highly accurate. \n",
            "‚Ä¢ Build Your Own RAG System: Follow a step-by-step guide to develop a complete LLM-RAG \n",
            "pipeline‚Äîfrom setting up a retrieval database and generating document embeddings to\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Preview the chunks\n",
        "for i, chunk in enumerate(all_splits[:2]):  # print first 2 for brevity\n",
        "    print(f\"--- üìë Chunk {i+1} ---\")\n",
        "    print(chunk.page_content) \n",
        "    #print(\"Metadata:\", chunk.metadata)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHjxcY36DCc_"
      },
      "source": [
        "### üîç Convert Text Chunks to Embeddings\n",
        "\n",
        "We now convert each text chunk into a high-dimensional vector using an embedding model. These vectors capture the semantic meaning of the text.\n",
        "\n",
        "- We use `HuggingFaceBgeEmbeddings from LangChain`.\n",
        "\n",
        "- Normalizing embeddings helps improve similarity search accuracy.\n",
        "\n",
        "- We set the device to \"cpu\" for compatibility with Colab. (If you're running this on a local machine with GPU, you can switch \"cpu\" to \"cuda\" for better performance.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "x0UZzf5GMQ0c"
      },
      "outputs": [],
      "source": [
        "# Define the embedding model \n",
        "EMBEDDING_MODEL_NAME = \"Qwen/Qwen3-Embedding-0.6B\"\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL_NAME,\n",
        "    model_kwargs={\"device\": \"cpu\"},  # \"cuda\" if you run locally with a GPU\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example - Create one vector from a text\n",
        "sample_text = \"...\"\n",
        "vec = embedding_model.embed_query(sample_text)\n",
        "\n",
        "print(\"Vector length:\", len(vec))\n",
        "print(\"First 10 values:\", vec[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kDIcoz_DWI4"
      },
      "source": [
        "### üèóÔ∏è Create and Save the Vectorstore\n",
        "\n",
        "Using the text chunks and embeddings, we build our vectorstore:\n",
        "\n",
        "- FAISS (Facebook AI Similarity Search) is a fast library for vector similarity search.\n",
        "\n",
        "- This index will let us retrieve the most relevant chunks given a user question.\n",
        "\n",
        "We also save the vectorstore locally so that it can be reused later without recomputing everything."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4XZVZMpNKmC",
        "outputId": "21c28fdb-bab9-4cb6-e792-9c7c5e14affe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Vectorstore created and saved successfully.\n"
          ]
        }
      ],
      "source": [
        "# Create a FAISS index from the text chunks and their embeddings\n",
        "vectorstore = FAISS.from_documents(documents=all_splits, embedding=embedding_model)\n",
        "\n",
        "# Save the vectorstore locally for reuse\n",
        "vectorstore.save_local(VECTORSTORES_DIR)\n",
        "\n",
        "print(\"‚úÖ Vectorstore created and saved successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n5XOWboD24O"
      },
      "source": [
        "üíæ Reload the Vectorstore (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whi6g_8TEEoJ",
        "outputId": "e183a27d-d437-49f3-fd0e-19fea68185c3"
      },
      "outputs": [],
      "source": [
        "# You can reload the saved vectorstore anytime without recomputing everything\n",
        "vectorstore = FAISS.load_local(\n",
        "    VECTORSTORES_DIR,\n",
        "    embedding_model,\n",
        "    allow_dangerous_deserialization=True  # Required in Colab environments\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Vectorstore reloaded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test a similarity search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîé Result 1\n",
            "integrating these components with an open source LLM. \n",
            "‚Ä¢ Gain Hands -On Experience:  Work through practical exercises and real -world examples \n",
            "that demonstrate how to efficiently search extensive documentation and leverage that data \n",
            "to inform precise responses. \n",
            "‚Ä¢ Ensure Data Security: Discover ef\n",
            "\n",
            "üîé Result 2\n",
            "Open Source AI workshop \n",
            "May 8, 2025, 9:00 ‚Äì Bern University of Applied Science (BFH) \n",
            " \n",
            "LLM-RAG from Scratch: Crafting Intelligent AI Retrieval Systems \n",
            "Embark on a hands-on journey to build your very own Retrieval -Augmented Generation (LLM-RAG) \n",
            "system from scratch using open source AI models and\n"
          ]
        }
      ],
      "source": [
        "query = \"...\"\n",
        "\n",
        "# Wrap FAISS as a retriever\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "# Retrieve top-k results\n",
        "results_faiss = retriever.get_relevant_documents(query)\n",
        "\n",
        "# Print them\n",
        "for i, res in enumerate(results_faiss, 1):\n",
        "    print(f\"\\nüîé Result {i}\")\n",
        "    print(res.page_content)  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### üîπ Minimal example: see BM25 sparse vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"...\"\n",
        "\n",
        "# Create BM25 retriever with top-k limit\n",
        "retriever = BM25Retriever.from_documents(all_splits)\n",
        "retriever.k = 2   # limit results to top-2\n",
        "\n",
        "results_BM25 = retriever.get_relevant_documents(query)\n",
        "\n",
        "for i, res in enumerate(results_BM25, 1):\n",
        "    print(f\"\\nüîé Result {i}\")\n",
        "    print(res.page_content)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze4WmNqKD-Zd"
      },
      "source": [
        "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
