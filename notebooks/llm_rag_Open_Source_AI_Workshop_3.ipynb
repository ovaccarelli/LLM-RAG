{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ovaccarelli/LLM-RAG/blob/main/notebooks/llm_rag_Open_Source_AI_Workshop_3.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0MJe6AusVtI"
      },
      "source": [
        "\n",
        "# üîß Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install all required Python packages for this workshop\n",
        "\n",
        "!pip install wget langchain langchain-community pypdf faiss-cpu sentence-transformers rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import wget\n",
        "from rich.console import Console\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings, HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.retrievers import BM25Retriever\n",
        "\n",
        "console = Console()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPoWurPW_5gb"
      },
      "source": [
        "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKgX3jOJ7OZT"
      },
      "source": [
        "## 3. Construct the vectorstore\n",
        "\n",
        "In this step, we take the PDF documents and transform them into a searchable vector database.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the \"data/PDFs\" folder if it doesn't exist\n",
        "PDF_FOLDER = Path(\"../data/PDFs\")\n",
        "os.makedirs(PDF_FOLDER, exist_ok=True)\n",
        "\n",
        "urls = [\n",
        "    \"https://raw.githubusercontent.com/ovaccarelli/LLM-RAG/main/data/PDFs/Open_Source_AI_workshop.pdf\",\n",
        "]\n",
        "\n",
        "# Download the PDFs\n",
        "for url in urls:\n",
        "    name = url.split(\"/\")[-1]\n",
        "    if not (PDF_FOLDER / name).is_file():\n",
        "        filename = wget.download(url, f\"data/PDFs/{name}\")\n",
        "console.print(\"Pdf file downloaded successfully.\", style=\"bold green\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0IUq-0eF1_x",
        "outputId": "babd1cf4-4e4e-492b-e52e-e72f0d2f8078"
      },
      "outputs": [],
      "source": [
        "# 1. Create a folder to store the vector index\n",
        "VECTORSTORES_DIR = Path(\"../data/vectorstores\")\n",
        "os.makedirs(VECTORSTORES_DIR, exist_ok=True)\n",
        "\n",
        "# 2. Point to the directory containing our PDFs\n",
        "PDF_FOLDER = Path(\"../data/PDFs\")\n",
        "\n",
        "# 3. Use PyPDFDirectoryLoader to load every PDF page as a Document\n",
        "loader = PyPDFDirectoryLoader(PDF_FOLDER)\n",
        "documents = loader.load()\n",
        "\n",
        "# 4. Verify how many pages are loaded\n",
        "print(f\"Loaded {len(documents)} PDF pages\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U69xtJTpC0uk"
      },
      "source": [
        "### ‚úÇÔ∏è Split Documents into Chunks\n",
        "\n",
        "We break documents into smaller overlapping chunks using `RecursiveCharacterTextSplitter`.\n",
        "\n",
        "- `chunk_size`: The number of characters per chunk.\n",
        "\n",
        "- `chunk_overlap`: Ensures that we maintain context between chunks.\n",
        "\n",
        "This is crucial for preserving semantic meaning across sentences and paragraphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u2AIMgmMNHw",
        "outputId": "df3f34f8-2aaf-497f-ec9e-8e87df2db812"
      },
      "outputs": [],
      "source": [
        "# Set chunk size (how many characters per chunk) and overlap\n",
        "CHUNK_SIZE = 500\n",
        "CHUNK_OVERLAP = 10\n",
        "\n",
        "# Initialize the text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=CHUNK_OVERLAP\n",
        ")\n",
        "\n",
        "# Split the loaded PDFs into smaller, overlapping chunks\n",
        "all_splits = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"‚úÖ Split into {len(all_splits)} chunks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview the chunks\n",
        "for i, chunk in enumerate(all_splits[:2]):  # print first 2 for brevity\n",
        "    print(f\"--- üìë Chunk {i+1} ---\")\n",
        "    print(chunk.page_content) \n",
        "    #print(\"Metadata:\", chunk.metadata)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHjxcY36DCc_"
      },
      "source": [
        "### üîç Convert Text Chunks to Embeddings\n",
        "\n",
        "We now convert each text chunk into a high-dimensional vector using an embedding model. These vectors capture the semantic meaning of the text.\n",
        "\n",
        "- We use `HuggingFaceBgeEmbeddings from LangChain`.\n",
        "\n",
        "- Normalizing embeddings helps improve similarity search accuracy.\n",
        "\n",
        "- We set the device to \"cpu\" for compatibility with Colab. (If you're running this on a local machine with GPU, you can switch \"cpu\" to \"cuda\" for better performance.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0UZzf5GMQ0c"
      },
      "outputs": [],
      "source": [
        "# Define the embedding model \n",
        "EMBEDDING_MODEL_NAME = \"Qwen/Qwen3-Embedding-0.6B\"\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL_NAME,\n",
        "    model_kwargs={\"device\": \"cpu\"},  # \"cuda\" if you run locally with a GPU\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example - Create one vector from a text\n",
        "sample_text = \"...\"\n",
        "vec = embedding_model.embed_query(sample_text)\n",
        "\n",
        "print(\"Vector length:\", len(vec))\n",
        "print(\"First 10 values:\", vec[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kDIcoz_DWI4"
      },
      "source": [
        "### üèóÔ∏è Create and Save the Vectorstore\n",
        "\n",
        "Using the text chunks and embeddings, we build our vectorstore:\n",
        "\n",
        "- FAISS (Facebook AI Similarity Search) is a fast library for vector similarity search.\n",
        "\n",
        "- This index will let us retrieve the most relevant chunks given a user question.\n",
        "\n",
        "We also save the vectorstore locally so that it can be reused later without recomputing everything."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4XZVZMpNKmC",
        "outputId": "21c28fdb-bab9-4cb6-e792-9c7c5e14affe"
      },
      "outputs": [],
      "source": [
        "# Create a FAISS index from the text chunks and their embeddings\n",
        "vectorstore = FAISS.from_documents(documents=all_splits, embedding=embedding_model)\n",
        "\n",
        "# Save the vectorstore locally for reuse\n",
        "vectorstore.save_local(VECTORSTORES_DIR)\n",
        "\n",
        "print(\"‚úÖ Vectorstore created and saved successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n5XOWboD24O"
      },
      "source": [
        "üíæ Reload the Vectorstore (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whi6g_8TEEoJ",
        "outputId": "e183a27d-d437-49f3-fd0e-19fea68185c3"
      },
      "outputs": [],
      "source": [
        "# You can reload the saved vectorstore anytime without recomputing everything\n",
        "vectorstore = FAISS.load_local(\n",
        "    VECTORSTORES_DIR,\n",
        "    embedding_model,\n",
        "    allow_dangerous_deserialization=True  # Required in Colab environments\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Vectorstore reloaded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test a similarity search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"...\"\n",
        "results_faiss = vectorstore.similarity_search(query, k=2)\n",
        "\n",
        "for i, res in enumerate(results_faiss, 1):\n",
        "    print(f\"\\nüîé Result {i}\")\n",
        "    print(res.page_content)  # preview chunk text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### üîπ Minimal example: see BM25 sparse vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"...\"\n",
        "\n",
        "# Create BM25 retriever with top-k limit\n",
        "retriever = BM25Retriever.from_documents(all_splits)\n",
        "retriever.k = 2   # limit results to top-2\n",
        "\n",
        "results_BM25 = retriever.get_relevant_documents(query)\n",
        "\n",
        "for i, res in enumerate(results_BM25, 1):\n",
        "    print(f\"\\nüîé Result {i}\")\n",
        "    print(res.page_content[:300])  # preview chunk text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze4WmNqKD-Zd"
      },
      "source": [
        "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
