{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ovaccarelli/LLM-RAG/blob/main/notebooks/llm_rag_Open_Source_AI_Workshop_3.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0MJe6AusVtI"
      },
      "source": [
        "\n",
        "# üîß 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install all required Python packages for this workshop\n",
        "\n",
        "!pip install wget langchain langchain-community pypdf faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import wget\n",
        "from rich.console import Console\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "console = Console()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPoWurPW_5gb"
      },
      "source": [
        "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKgX3jOJ7OZT"
      },
      "source": [
        "## 4. Construct the vectorstore\n",
        "\n",
        "In this step, we take the PDF documents and transform them into a searchable vector database.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the \"data/PDFs\" folder if it doesn't exist\n",
        "PDF_FOLDER = Path(\"data/PDFs\")\n",
        "os.makedirs(PDF_FOLDER, exist_ok=True)\n",
        "\n",
        "urls = [\n",
        "    \"https://raw.githubusercontent.com/ovaccarelli/LLM-RAG/main/data/PDFs/Open_Source_AI_workshop.pdf\",\n",
        "]\n",
        "\n",
        "# Download the PDFs\n",
        "for url in urls:\n",
        "    name = url.split(\"/\")[-1]\n",
        "    if not (PDF_FOLDER / name).is_file():\n",
        "        filename = wget.download(url, f\"data/PDFs/{name}\")\n",
        "console.print(\"Pdf file downloaded successfully.\", style=\"bold green\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0IUq-0eF1_x",
        "outputId": "babd1cf4-4e4e-492b-e52e-e72f0d2f8078"
      },
      "outputs": [],
      "source": [
        "# 1. Create a folder to store the vector index\n",
        "VECTORSTORES_DIR = Path(\"data/vectorstores\")\n",
        "os.makedirs(VECTORSTORES_DIR, exist_ok=True)\n",
        "\n",
        "# 2. Point to the directory containing our PDFs\n",
        "PDF_FOLDER = Path(\"data/PDFs\")\n",
        "\n",
        "# 3. Use PyPDFDirectoryLoader to load every PDF page as a Document\n",
        "loader = PyPDFDirectoryLoader(PDF_FOLDER)\n",
        "documents = loader.load()\n",
        "\n",
        "# 4. Verify how many pages are loaded\n",
        "print(f\"Loaded {len(documents)} PDF pages\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U69xtJTpC0uk"
      },
      "source": [
        "### ‚úÇÔ∏è Split Documents into Chunks\n",
        "\n",
        "We break documents into smaller overlapping chunks using `RecursiveCharacterTextSplitter`.\n",
        "\n",
        "- `chunk_size`: The number of characters per chunk.\n",
        "\n",
        "- `chunk_overlap`: Ensures that we maintain context between chunks.\n",
        "\n",
        "This is crucial for preserving semantic meaning across sentences and paragraphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u2AIMgmMNHw",
        "outputId": "df3f34f8-2aaf-497f-ec9e-8e87df2db812"
      },
      "outputs": [],
      "source": [
        "# Set chunk size (how many characters per chunk) and overlap\n",
        "CHUNK_SIZE = ...\n",
        "CHUNK_OVERLAP = ...\n",
        "\n",
        "# Initialize the text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=CHUNK_OVERLAP\n",
        ")\n",
        "\n",
        "# Split the loaded PDFs into smaller, overlapping chunks\n",
        "all_splits = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"‚úÖ Split into {len(all_splits)} chunks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHjxcY36DCc_"
      },
      "source": [
        "### üîç Convert Text Chunks to Embeddings\n",
        "\n",
        "We now convert each text chunk into a high-dimensional vector using the BGE model (`BAAI/bge-large-en-v1.5`). These vectors capture the semantic meaning of the text.\n",
        "\n",
        "- We use `HuggingFaceBgeEmbeddings from LangChain`.\n",
        "\n",
        "- Normalizing embeddings helps improve similarity search accuracy.\n",
        "\n",
        "- We set the device to \"cpu\" for compatibility with Colab. (If you're running this on a local machine with GPU, you can switch \"cpu\" to \"cuda\" for better performance.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0UZzf5GMQ0c"
      },
      "outputs": [],
      "source": [
        "# Define the embedding model ‚Äî BGE is a strong open-source embedding model for English\n",
        "EMBEDDING_MODEL_NAME = \"BAAI/bge-large-en-v1.5\"\n",
        "\n",
        "embedding_model = HuggingFaceBgeEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL_NAME,\n",
        "    model_kwargs={\"device\": \"cpu\"},  # \"cuda\" if you run locally with a GPU\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kDIcoz_DWI4"
      },
      "source": [
        "### üèóÔ∏è Create and Save the Vectorstore\n",
        "\n",
        "Using the text chunks and embeddings, we build our vectorstore:\n",
        "\n",
        "- FAISS (Facebook AI Similarity Search) is a fast library for vector similarity search.\n",
        "\n",
        "- This index will let us retrieve the most relevant chunks given a user question.\n",
        "\n",
        "We also save the vectorstore locally so that it can be reused later without recomputing everything."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4XZVZMpNKmC",
        "outputId": "21c28fdb-bab9-4cb6-e792-9c7c5e14affe"
      },
      "outputs": [],
      "source": [
        "# Create a FAISS index from the text chunks and their embeddings\n",
        "vectorstore = FAISS.from_documents(documents=all_splits, embedding=embedding_model)\n",
        "\n",
        "# Save the vectorstore locally for reuse\n",
        "vectorstore.save_local(VECTORSTORES_DIR)\n",
        "\n",
        "print(\"‚úÖ Vectorstore created and saved successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n5XOWboD24O"
      },
      "source": [
        "üíæ Reload the Vectorstore (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whi6g_8TEEoJ",
        "outputId": "e183a27d-d437-49f3-fd0e-19fea68185c3"
      },
      "outputs": [],
      "source": [
        "# You can reload the saved vectorstore anytime without recomputing everything\n",
        "vectorstore = FAISS.load_local(\n",
        "    VECTORSTORES_DIR,\n",
        "    embedding_model,\n",
        "    allow_dangerous_deserialization=True  # Required in Colab environments\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Vectorstore reloaded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze4WmNqKD-Zd"
      },
      "source": [
        "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
